{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f64ab0ae-c9b6-41e0-94de-2ac625023479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Cuaderno de ingesta de datos\n",
    "En este bloque traeremosla información desde datos abiertos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f7eb04d-03a0-4e75-83d0-172b52bb09b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 1: Leer datos desde las APIs\n",
    "url_secop = \"/Volumes/main/diplmado_datos/manual/df_secop.csv\"\n",
    "url_men = \"/Volumes/main/diplmado_datos/manual/df_men.csv\"\n",
    "\n",
    "# Usamos spark.read para ingerir los datos.\n",
    "# option(\"header\", \"true\") indica que la primera fila contiene los nombres de las columnas.\n",
    "# option(\"inferSchema\", \"true\") le pide a Spark que adivine el tipo de cada columna.\n",
    "df_secop = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(url_secop)\n",
    "df_men = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(url_men)\n",
    "\n",
    "# display() es un comando especial de Databricks para una visualización interactiva.\n",
    "print(\"Datos del SECOP cargados:\")\n",
    "display(df_secop)\n",
    "\n",
    "print(\"Datos del MEN cargados:\")\n",
    "display(df_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9765adc0-fbeb-4251-a012-00700be8d0f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 2: Guardar los DataFrames como tablas Delta\n",
    "# La función .saveAsTable() guarda los datos y registra la tabla en el Unity Catalog.\n",
    "# El modo \"overwrite\" reemplaza la tabla si ya existe, ideal para actualizaciones.\n",
    "df_secop.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplmado_datos.secop\")\n",
    "df_men.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"main.diplmado_datos.men_estadisticas\")\n",
    "\n",
    "print(\"¡Tablas guardadas exitosamente en el catálogo 'main', esquema 'diplmado_datos'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b738eb83-2ff9-47d3-9d57-8242581d44fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Descargar los datos con requests y leerlos en pandas\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "url_secop = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit=100000\"\n",
    "url_men = \"https://www.datos.gov.co/resource/nudc-7mev.csv?$limit=100000\"\n",
    "\n",
    "# Descargar contenido\n",
    "response_secop = requests.get(url_secop)\n",
    "response_men = requests.get(url_men)\n",
    "\n",
    "# Convertir contenido a pandas usando StringIO\n",
    "df_secop_pd = pd.read_csv(StringIO(response_secop.text))\n",
    "df_men_pd = pd.read_csv(StringIO(response_men.text))\n",
    "\n",
    "# Convertir pandas a Spark\n",
    "df_secop = spark.createDataFrame(df_secop_pd)\n",
    "df_men = spark.createDataFrame(df_men_pd)\n",
    "\n",
    "# Mostrar en Databricks\n",
    "display(df_secop)\n",
    "display(df_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b54f6109-2872-417c-83a8-935e82fbd3a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paso 1: Descargar los datos con requests y leerlos en pandas\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "url_secop = \"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit=100000&$offset=100000\"\n",
    "\n",
    "# Descargar contenido\n",
    "response_secop = requests.get(url_secop)\n",
    "\n",
    "# Convertir contenido a pandas usando StringIO\n",
    "df_secop_pd = pd.read_csv(StringIO(response_secop.text), delimiter=',', low_memory=False)\n",
    "\n",
    "# limpiar nombres de columnas (opcional pero recomendado)\n",
    "df_secop_pd.columns = [col.strip().lower().replace(' ', '_') for col in df_secop_pd.columns]\n",
    "\n",
    "# Verifica que los datos se cargaron correctamente en pandas \n",
    "print(df_secop_pd.head())\n",
    "\n",
    "# Convertir pandas a Spark\n",
    "df_secop = spark.createDataFrame(df_secop_pd)\n",
    "\n",
    "# Mostrar en Databricks\n",
    "display(df_secop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b41c07-99d8-4ee9-9b42-244ce5a3057a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd947fa-ce39-4dc0-b6e3-cfe210db8215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "target_schema = spark.table(\"main.diplmado_datos.secop\").schema\n",
    "\n",
    "df_secop_aligned = df_secop.select(\n",
    "    [col(field.name).cast(field.dataType) for field in target_schema.fields if field.name in df_secop.columns]\n",
    ")\n",
    "\n",
    "df_secop_aligned.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"main.diplmado_datos.secop\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7436d0bf-ebf5-4ac7-835f-484ed0c48d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# registros en SECOP\n",
    "total_registros = 19446266\n",
    "# registros actuales 200.000\n",
    "offset_inicial = 200000\n",
    "# tamaño\n",
    "limite = 100000\n",
    "\n",
    "# bloques faltantes \n",
    "paginas_faltantes = ((total_registros - offset_inicial) // limite) + 1\n",
    "\n",
    "print(f\"Quedan {paginas_faltantes} bloques por descargar...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f997fa-5935-4cf7-a53c-5e65d4cec9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Crear sesión Spark (si no existe)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Parámetros\n",
    "total_registros = 19_449_349   # Total estimado de registros en SECOP\n",
    "limite = 50_000                # Límite máximo por descarga (seguro con datos.gov.co)\n",
    "paginas = (total_registros // limite) + 1\n",
    "\n",
    "print(f\"Descargando {total_registros:,} registros en {paginas} páginas...\")\n",
    "\n",
    "for i in range(paginas):\n",
    "    offset = i * limite\n",
    "    url_secop = f\"https://www.datos.gov.co/resource/rpmr-utcd.csv?$limit={limite}&$offset={offset}\"\n",
    "\n",
    "    print(f\"→ Página {i+1}/{paginas} (offset={offset})\")\n",
    "\n",
    "    response = requests.get(url_secop)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        df_pd = pd.read_csv(StringIO(response.text), dtype=str, low_memory=False)\n",
    "\n",
    "        if df_pd.empty:\n",
    "            print(f\"Página {i+1} vacía. Deteniendo descarga.\")\n",
    "            break\n",
    "\n",
    "        # Normalizar nombres de columnas (opcional)\n",
    "        df_pd.columns = [col.strip().lower().replace(' ', '_') for col in df_pd.columns]\n",
    "\n",
    "        # Convertir a Spark\n",
    "        df_spark = spark.createDataFrame(df_pd)\n",
    "\n",
    "        # Alinear con el esquema si ya existe la tabla\n",
    "        if i == 0:\n",
    "            # Primera página: sobrescribimos y registramos tabla\n",
    "            df_spark.write.format(\"delta\") \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(\"main.diplmado_datos.secop\")\n",
    "        else:\n",
    "            # Páginas siguientes: alineamos esquema y hacemos append\n",
    "            target_schema = spark.table(\"main.diplmado_datos.secop\").schema\n",
    "            df_aligned = df_spark.select(\n",
    "                [col(field.name).cast(field.dataType) for field in target_schema.fields if field.name in df_spark.columns]\n",
    "            )\n",
    "\n",
    "            df_aligned.write.format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .saveAsTable(\"main.diplmado_datos.secop\")\n",
    "\n",
    "        print(f\"✓ Página {i+1} guardada con {len(df_pd):,} filas.\")\n",
    "    else:\n",
    "        print(f\"Error HTTP {response.status_code} en página {i+1}. Deteniendo proceso.\")\n",
    "        break\n",
    "\n",
    "print(\"¡Carga completa de SECOP en Delta!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f8a306-1de6-4887-ba1b-0e5fe91b66bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "token = dbutils.widgets.get(\"token_app\")\n",
    "print(token)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6642975633027921,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingesta_Datos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
